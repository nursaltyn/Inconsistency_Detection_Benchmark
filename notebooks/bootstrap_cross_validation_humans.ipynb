{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we estimate:\n",
    "\n",
    "- Upper bound of performance on Inconsistency detection (referred to as Bootstrap)\n",
    "- Individual human performance at predicting the majority label (referred to as Cross-validation)\n",
    "\n",
    "Additionally:\n",
    "\n",
    "- \"Majority LLM\" means we are evaluating LLMs based on their majority class out of the 5 runs.\n",
    "- \"5 runs\" means we are treating each of the 5 LLM runs as a separate prediction.\n",
    "\n",
    "We run the analysis for both 5 and 3 classes settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = pd.read_csv(r\"../data/qualtrics_survey/results/df_gold_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_5 = ['Unrelated', 'Consistent', 'Indirect inconsistency', \"Factual inconsistency\", 'Surface contradiction']                             \n",
    "classes_3 = ['Unrelated', 'Consistent', 'Inconsistent']\n",
    "classes_5_to_3 = {'Unrelated': 'Unrelated', 'Consistent': 'Consistent', 'Indirect inconsistency': 'Inconsistent', \"Factual inconsistency\": 'Inconsistent', 'Surface contradiction': 'Inconsistent'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic upper-bound\n",
    "\n",
    "#### Bootstrap 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_predictions_tuples = []\n",
    "\n",
    "for i in range(len(df_gold)):\n",
    "    # we have 5 tuples per sample \n",
    "    strings = eval(df_gold['All answers'][i])\n",
    "    # create N-1 tuples\n",
    "    versions = [(strings[j], strings[:j] + strings[j+1:]) for j in range(len(strings))]\n",
    "    # print(len(versions))\n",
    "    # go through each element, value is held out \"ground truth\"\n",
    "    for value, version in versions:\n",
    "        list_annotator_dropped = version\n",
    "        counter = Counter(list_annotator_dropped)\n",
    "        # identify the majority vote and sample randomly if more than 1\n",
    "        max_count = max(counter.values())\n",
    "        top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "        # this is today's ground truth\n",
    "        majority = random.choice(top_labels)\n",
    "        # 10 bootstraps of the same pool where we took the majority from \n",
    "        bootstraps = [random.choices(version, k=len(version)) for _ in range(10)]\n",
    "        bootstrap_majorities = []\n",
    "        for bootstrap in bootstraps:\n",
    "            counter = Counter(bootstrap)\n",
    "            max_count = max(counter.values())\n",
    "            top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "            # this is tomorrow's ground truth\n",
    "            tomorrow_majority = random.choice(top_labels)\n",
    "            bootstrap_majorities.append(tomorrow_majority)\n",
    "        # print(len(bootstrap_majorities))\n",
    "        to_append = []\n",
    "        for tomorrow_majority in bootstrap_majorities:\n",
    "            to_append.append((majority, tomorrow_majority))\n",
    "        bootstrap_predictions_tuples.extend(to_append)        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/bootstrap_predictions_tuples.pkl', 'wb') as f:\n",
    "    pickle.dump(bootstrap_predictions_tuples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_predictions_tuples_3_classes = []\n",
    "\n",
    "for i in range(len(df_gold)):\n",
    "    # we have 5 tuples per sample \n",
    "    strings = eval(df_gold['All answers'][i])\n",
    "    # create N-1 tuples\n",
    "    versions = [(strings[j], strings[:j] + strings[j+1:]) for j in range(len(strings))]\n",
    "    # print(len(versions))\n",
    "    # go through each element, value is held out \"ground truth\"\n",
    "    for value, version in versions:\n",
    "        list_annotator_dropped = [classes_5_to_3[el] for el in version]\n",
    "        version = [classes_5_to_3[el] for el in version]\n",
    "        value = classes_5_to_3[value]\n",
    "        counter = Counter(list_annotator_dropped)\n",
    "        # identify the majority vote and sample randomly if more than 1\n",
    "        max_count = max(counter.values())\n",
    "        top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "        # this is today's ground truth\n",
    "        majority = random.choice(top_labels)\n",
    "        # 10 bootstraps of the same pool where we took the majority from \n",
    "        bootstraps = [random.choices(version, k=len(version)) for _ in range(10)]\n",
    "        bootstrap_majorities = []\n",
    "        for bootstrap in bootstraps:\n",
    "            counter = Counter(bootstrap)\n",
    "            max_count = max(counter.values())\n",
    "            top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "            # this is tomorrow's ground truth\n",
    "            tomorrow_majority = random.choice(top_labels)\n",
    "            bootstrap_majorities.append(tomorrow_majority)\n",
    "        # print(len(bootstrap_majorities))\n",
    "        to_append = []\n",
    "        for tomorrow_majority in bootstrap_majorities:\n",
    "            to_append.append((majority, tomorrow_majority))\n",
    "        bootstrap_predictions_tuples_3_classes.extend(to_append)        \n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/bootstrap_predictions_tuples_3_classes.pkl', 'wb') as f:\n",
    "    pickle.dump(bootstrap_predictions_tuples_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation: majority LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ground_truth_tuples_majority_LLM = dict()\n",
    "prediction_ground_truth_tuples_majority_LLM['Humans'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM['ChatGPT-4'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM['ChatGPT-3.5'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM['LLaMA 8B'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM['LLaMA 70B'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_gold)):\n",
    "    # we have 5 tuples per sample \n",
    "    # ground truth - from majority, the 5th one is prediction \n",
    "    strings = eval(df_gold['All answers'][i])\n",
    "    # create N tuples of size (N-1), 1 held-out\n",
    "    versions = [(strings[j], strings[:j] + strings[j+1:]) for j in range(len(strings))]\n",
    "    # go through each element\n",
    "    # value is held out \"prediction\"\n",
    "    for value, version in versions:\n",
    "        list_annotator_dropped = version\n",
    "        counter = Counter(list_annotator_dropped)\n",
    "        # identify the majority vote and sample randomly if more than 1\n",
    "        max_count = max(counter.values())\n",
    "        top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "        majority = random.choice(top_labels)\n",
    "        prediction_ground_truth_tuples_majority_LLM['Humans'].append((majority, value)) #ground truth, prediction\n",
    "        prediction_ground_truth_tuples_majority_LLM['ChatGPT-4'].append((majority, df_gold['ChatGPT-4 majority'][i]))\n",
    "        prediction_ground_truth_tuples_majority_LLM['ChatGPT-3.5'].append((majority, df_gold['ChatGPT-3.5 majority'][i]))\n",
    "        prediction_ground_truth_tuples_majority_LLM['LLaMA 8B'].append((majority, df_gold['Llama 8B majority'][i]))\n",
    "        prediction_ground_truth_tuples_majority_LLM['LLaMA 70B'].append((majority, df_gold['Llama 70B majority'][i]))\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/prediction_ground_truth_tuples_majority_LLM.json', 'w') as f:\n",
    "    json.dump(prediction_ground_truth_tuples_majority_LLM, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ground_truth_tuples_majority_LLM_3_classes = dict()\n",
    "prediction_ground_truth_tuples_majority_LLM_3_classes['Humans'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM_3_classes['ChatGPT-4'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM_3_classes['ChatGPT-3.5'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM_3_classes['LLaMA 8B'] = []\n",
    "prediction_ground_truth_tuples_majority_LLM_3_classes['LLaMA 70B'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_gold)):\n",
    "    # we have 5 tuples per sample \n",
    "    # ground truth - from majority, the 5th one is prediction \n",
    "    strings = eval(df_gold['All answers'][i])\n",
    "    # create N tuples of size (N-1), 1 held-out\n",
    "    versions = [(strings[j], strings[:j] + strings[j+1:]) for j in range(len(strings))]\n",
    "    # go through each element\n",
    "    # value is held out \"prediction\"\n",
    "    for value, version in versions:\n",
    "        list_annotator_dropped = [classes_5_to_3[el] for el in version]\n",
    "        value = classes_5_to_3[value]\n",
    "        counter = Counter(list_annotator_dropped)\n",
    "        # identify the majority vote and sample randomly if more than 1\n",
    "        max_count = max(counter.values())\n",
    "        top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "        majority = random.choice(top_labels)\n",
    "        prediction_ground_truth_tuples_majority_LLM_3_classes['Humans'].append((majority, value)) #ground truth, prediction\n",
    "        prediction_ground_truth_tuples_majority_LLM_3_classes['ChatGPT-4'].append((majority, classes_5_to_3[df_gold['ChatGPT-4 majority'][i]]))\n",
    "        prediction_ground_truth_tuples_majority_LLM_3_classes['ChatGPT-3.5'].append((majority, classes_5_to_3[df_gold['ChatGPT-3.5 majority'][i]]))\n",
    "        prediction_ground_truth_tuples_majority_LLM_3_classes['LLaMA 8B'].append((majority, classes_5_to_3[df_gold['Llama 8B majority'][i]]))\n",
    "        prediction_ground_truth_tuples_majority_LLM_3_classes['LLaMA 70B'].append((majority, classes_5_to_3[df_gold['Llama 70B majority'][i]]))\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/prediction_ground_truth_tuples_majority_LLM_3_classes.json', 'w') as f:\n",
    "    json.dump(prediction_ground_truth_tuples_majority_LLM_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation for all 5 runs - 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ground_truth_tuples_5_runs_3_classes = dict()\n",
    "prediction_ground_truth_tuples_5_runs_3_classes['Humans'] = []\n",
    "prediction_ground_truth_tuples_5_runs_3_classes['ChatGPT-4'] = []\n",
    "prediction_ground_truth_tuples_5_runs_3_classes['ChatGPT-3.5'] = []\n",
    "prediction_ground_truth_tuples_5_runs_3_classes['LLaMA 8B'] = []\n",
    "prediction_ground_truth_tuples_5_runs_3_classes['LLaMA 70B'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_gold)):\n",
    "    # we have 5 tuples per sample \n",
    "    # ground truth - from majority, the 5th one is prediction \n",
    "    strings = eval(df_gold['All answers'][i])\n",
    "    # create N tuples of size (N-1), 1 held-out\n",
    "    versions = [(strings[j], strings[:j] + strings[j+1:]) for j in range(len(strings))]\n",
    "    # go through each element\n",
    "    # value is held out \"prediction\"\n",
    "    for value, version in versions:\n",
    "        list_annotator_dropped = [classes_5_to_3[el] for el in version]\n",
    "        counter = Counter(list_annotator_dropped)\n",
    "        # identify the majority vote and sample randomly if more than 1\n",
    "        max_count = max(counter.values())\n",
    "        top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "        majority = random.choice(top_labels)\n",
    "        value = classes_5_to_3[value]\n",
    "        prediction_ground_truth_tuples_5_runs_3_classes['Humans'].append((majority, value)) #ground truth, prediction\n",
    "\n",
    "        # compare each majority with each of the 5 runs results of the model \n",
    "        for run in range(5):\n",
    "            prediction_ground_truth_tuples_5_runs_3_classes['ChatGPT-4'].append((majority, classes_5_to_3[eval(df_gold['ChatGPT-4 5 runs'][i])[run]]))\n",
    "            prediction_ground_truth_tuples_5_runs_3_classes['ChatGPT-3.5'].append((majority, classes_5_to_3[eval(df_gold['ChatGPT-3.5 5 runs'][i])[run]]))\n",
    "            prediction_ground_truth_tuples_5_runs_3_classes['LLaMA 8B'].append((majority, classes_5_to_3[eval(df_gold['LLaMA 8B 5 runs'][i])[run]]))\n",
    "            prediction_ground_truth_tuples_5_runs_3_classes['LLaMA 70B'].append((majority, classes_5_to_3[eval(df_gold['LLaMA 70B 5 runs'][i])[run]]))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/prediction_ground_truth_tuples_5_runs_3_classes.json', 'w') as f:\n",
    "    json.dump(prediction_ground_truth_tuples_5_runs_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation for all 5 runs - 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ground_truth_tuples_5_runs = dict()\n",
    "prediction_ground_truth_tuples_5_runs['Humans'] = []\n",
    "prediction_ground_truth_tuples_5_runs['ChatGPT-4'] = []\n",
    "prediction_ground_truth_tuples_5_runs['ChatGPT-3.5'] = []\n",
    "prediction_ground_truth_tuples_5_runs['LLaMA 8B'] = []\n",
    "prediction_ground_truth_tuples_5_runs['LLaMA 70B'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_gold)):\n",
    "    # we have 5 tuples per sample \n",
    "    # ground truth - from majority, the 5th one is prediction \n",
    "    strings = eval(df_gold['All answers'][i])\n",
    "    # create N tuples of size (N-1), 1 held-out\n",
    "    versions = [(strings[j], strings[:j] + strings[j+1:]) for j in range(len(strings))]\n",
    "    # go through each element\n",
    "    # value is held out \"prediction\"\n",
    "    for value, version in versions:\n",
    "        list_annotator_dropped = version\n",
    "        counter = Counter(list_annotator_dropped)\n",
    "        # identify the majority vote and sample randomly if more than 1\n",
    "        max_count = max(counter.values())\n",
    "        top_labels = [label for label, count in counter.items() if count == max_count]\n",
    "        majority = random.choice(top_labels)\n",
    "        prediction_ground_truth_tuples_5_runs['Humans'].append((majority, value)) #ground truth, prediction\n",
    "\n",
    "        # compare each majority with each of the 5 runs results of the model \n",
    "        for run in range(5):\n",
    "            prediction_ground_truth_tuples_5_runs['ChatGPT-4'].append((majority, eval(df_gold['ChatGPT-4 5 runs'][i])[run]))\n",
    "            prediction_ground_truth_tuples_5_runs['ChatGPT-3.5'].append((majority, eval(df_gold['ChatGPT-3.5 5 runs'][i])[run]))\n",
    "            prediction_ground_truth_tuples_5_runs['LLaMA 8B'].append((majority, eval(df_gold['LLaMA 8B 5 runs'][i])[run]))\n",
    "            prediction_ground_truth_tuples_5_runs['LLaMA 70B'].append((majority, eval(df_gold['LLaMA 70B 5 runs'][i])[run]))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/prediction_ground_truth_tuples_5_runs.json', 'w') as f:\n",
    "    json.dump(prediction_ground_truth_tuples_5_runs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

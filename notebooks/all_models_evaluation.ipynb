{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate models and humans on the following metrics:\n",
    "\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score (macro)\n",
    "- Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "Additionally:\n",
    "\n",
    "- \"Majority LLM\" means we are evaluating LLMs based on their majority class out of the 5 runs.\n",
    "- \"5 runs\" means we are treating each of the 5 LLM runs as a separate prediction.\n",
    "\n",
    "We run the analysis for both 5 and 3 classes settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = pd.read_csv(r\"../data/df_gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_5 = ['Unrelated', 'Consistent', 'Indirect inconsistency', \"Factual inconsistency\", 'Surface contradiction']                             \n",
    "classes_3 = ['Unrelated', 'Consistent', 'Inconsistent']\n",
    "classes_5_to_3 = {'Unrelated': 'Unrelated', 'Consistent': 'Consistent', 'Indirect inconsistency': 'Inconsistent', \"Factual inconsistency\": 'Inconsistent', 'Surface contradiction': 'Inconsistent'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../data/model_evaluations/bootstrap_predictions_tuples.pkl\", 'rb') as f:\n",
    "    bootstrap_predictions_tuples = pickle.load(f)\n",
    "    \n",
    "with open(r\"../data/model_evaluations/bootstrap_predictions_tuples_3_classes.pkl\", 'rb') as f:\n",
    "    bootstrap_predictions_tuples_3_classes = pickle.load(f)\n",
    "    \n",
    "with open(r\"../data/model_evaluations/prediction_ground_truth_tuples_majority_LLM.json\", 'r') as f:\n",
    "    prediction_ground_truth_tuples_majority_LLM = json.load(f)\n",
    "    \n",
    "with open(r\"../data/model_evaluations/prediction_ground_truth_tuples_majority_LLM_3_classes.json\", 'r') as f:\n",
    "    prediction_ground_truth_tuples_majority_LLM_3_classes = json.load(f)\n",
    "    \n",
    "with open(r\"../data/model_evaluations/prediction_ground_truth_tuples_5_runs_3_classes.json\", 'r') as f:\n",
    "    prediction_ground_truth_tuples_5_runs_3_classes = json.load(f)\n",
    "    \n",
    "with open(r\"../data/model_evaluations/prediction_ground_truth_tuples_5_runs.json\", 'r') as f:\n",
    "    prediction_ground_truth_tuples_5_runs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Matthews Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "MCC for class 'Unrelated': 0.5118594119149651\n",
      "MCC for class 'Consistent': 0.6268537446684619\n",
      "MCC for class 'Indirect inconsistency': 0.24836592507889102\n",
      "MCC for class 'Factual inconsistency': 0.25585335049644997\n",
      "MCC for class 'Surface contradiction': 0.4175184199168447\n",
      "ChatGPT-4\n",
      "MCC for class 'Unrelated': 0.5478670649304451\n",
      "MCC for class 'Consistent': 0.6694946961029499\n",
      "MCC for class 'Indirect inconsistency': 0.1743969492825913\n",
      "MCC for class 'Factual inconsistency': 0.1834291017531791\n",
      "MCC for class 'Surface contradiction': 0.3280374205878848\n",
      "ChatGPT-3.5\n",
      "MCC for class 'Unrelated': 0.2826301125441576\n",
      "MCC for class 'Consistent': 0.4164564190994086\n",
      "MCC for class 'Indirect inconsistency': 0.08877300291709384\n",
      "MCC for class 'Factual inconsistency': 0.16703238114784744\n",
      "MCC for class 'Surface contradiction': 0.19384294648407363\n",
      "LLaMA 8B\n",
      "MCC for class 'Unrelated': 0.051258928298996816\n",
      "MCC for class 'Consistent': 0.24976100547798227\n",
      "MCC for class 'Indirect inconsistency': -0.021421919642153193\n",
      "MCC for class 'Factual inconsistency': -0.0018203001163354308\n",
      "MCC for class 'Surface contradiction': -0.0016689949427464933\n",
      "LLaMA 70B\n",
      "MCC for class 'Unrelated': 0.5105289903510936\n",
      "MCC for class 'Consistent': 0.6966480573098311\n",
      "MCC for class 'Indirect inconsistency': 0.2784448656073472\n",
      "MCC for class 'Factual inconsistency': 0.21483098875613774\n",
      "MCC for class 'Surface contradiction': 0.3879896657838228\n"
     ]
    }
   ],
   "source": [
    "matthews_corrcoef_majority_LLM_per_class = dict()\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_majority_LLM:\n",
    "    \n",
    "    print(model_type)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_majority_LLM[model_type]:\n",
    "        y_true.append(ground_truth)\n",
    "        y_pred.append(prediction)\n",
    "        \n",
    "    \n",
    "    # Compute MCC for each class\n",
    "    mcc_scores = []\n",
    "    for class_name in classes_5:\n",
    "        # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "        y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "        y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "        \n",
    "        # Calculate MCC for this class\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "\n",
    "    # Output MCC for each class\n",
    "    for class_name, mcc in zip(classes_5, mcc_scores):\n",
    "        print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "        \n",
    "    matthews_corrcoef_majority_LLM_per_class[model_type] = dict(zip(classes_5, mcc_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../data/model_evaluations/f1_score_per_model_type_majority_LLM.pkl\", 'rb') as f:\n",
    "    f1_score_per_model_type_majority_LLM = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC for class 'Unrelated': 0.7157325874971767\n",
      "MCC for class 'Consistent': 0.7857342723881804\n",
      "MCC for class 'Indirect inconsistency': 0.5906623958002127\n",
      "MCC for class 'Factual inconsistency': 0.57283963807414\n",
      "MCC for class 'Surface contradiction': 0.6749434614250672\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bootstrap'\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples:\n",
    "    y_true.append(ground_truth)\n",
    "    y_pred.append(prediction)\n",
    "    \n",
    "\n",
    "# Compute MCC for each class\n",
    "mcc_scores = []\n",
    "for class_name in classes_5:\n",
    "    # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "    y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "    y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "    \n",
    "    # Calculate MCC for this class\n",
    "    mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "# Output MCC for each class\n",
    "for class_name, mcc in zip(classes_5, mcc_scores):\n",
    "    print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "    \n",
    "matthews_corrcoef_majority_LLM_per_class[model_type] = dict(zip(classes_5, mcc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/matthews_corrcoef_majority_LLM_per_class.pkl', 'wb') as f:\n",
    "    pickle.dump(matthews_corrcoef_majority_LLM_per_class, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "MCC for class 'Unrelated': 0.5026503128019292\n",
      "MCC for class 'Consistent': 0.6367265146427177\n",
      "MCC for class 'Inconsistent': 0.6168235295892426\n",
      "ChatGPT-4\n",
      "MCC for class 'Unrelated': 0.5478002081755924\n",
      "MCC for class 'Consistent': 0.6624972248689089\n",
      "MCC for class 'Inconsistent': 0.6186233202710536\n",
      "ChatGPT-3.5\n",
      "MCC for class 'Unrelated': 0.3080388858278656\n",
      "MCC for class 'Consistent': 0.43242698976411825\n",
      "MCC for class 'Inconsistent': 0.3783360802867473\n",
      "LLaMA 8B\n",
      "MCC for class 'Unrelated': 0.06190079741535469\n",
      "MCC for class 'Consistent': 0.2451971829261946\n",
      "MCC for class 'Inconsistent': 0.10673700529267396\n",
      "LLaMA 70B\n",
      "MCC for class 'Unrelated': 0.5245552760933625\n",
      "MCC for class 'Consistent': 0.7073268914805231\n",
      "MCC for class 'Inconsistent': 0.6326792968688756\n",
      "MCC for class 'Unrelated': 0.7271250758242624\n",
      "MCC for class 'Consistent': 0.7982638493044966\n",
      "MCC for class 'Inconsistent': 0.786103408719816\n"
     ]
    }
   ],
   "source": [
    "matthews_corrcoef_majority_LLM_per_class_3_classes = dict()\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_majority_LLM_3_classes:\n",
    "    \n",
    "    print(model_type)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_majority_LLM_3_classes[model_type]:\n",
    "        y_true.append(ground_truth)\n",
    "        y_pred.append(prediction)\n",
    "            \n",
    "    # Compute MCC for each class\n",
    "    mcc_scores = []\n",
    "    for class_name in classes_3:\n",
    "        # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "        y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "        y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "        \n",
    "        # Calculate MCC for this class\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "\n",
    "    # Output MCC for each class\n",
    "    for class_name, mcc in zip(classes_3, mcc_scores):\n",
    "        print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "        \n",
    "    matthews_corrcoef_majority_LLM_per_class_3_classes[model_type] = dict(zip(classes_3, mcc_scores))\n",
    "\n",
    "\n",
    "model_type = 'bootstrap'\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples_3_classes:\n",
    "    y_true.append(ground_truth)\n",
    "    y_pred.append(prediction)\n",
    "    \n",
    "\n",
    "# Compute MCC for each class\n",
    "mcc_scores = []\n",
    "for class_name in classes_3:\n",
    "    # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "    y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "    y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "    \n",
    "    # Calculate MCC for this class\n",
    "    mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "# Output MCC for each class\n",
    "for class_name, mcc in zip(classes_3, mcc_scores):\n",
    "    print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "    \n",
    "matthews_corrcoef_majority_LLM_per_class_3_classes[model_type] = dict(zip(classes_3, mcc_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/matthews_corrcoef_majority_LLM_per_class_3_classes.pkl', 'wb') as f:\n",
    "    pickle.dump(matthews_corrcoef_majority_LLM_per_class_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCC: 5 runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "MCC for class 'Unrelated': 0.5200451910514584\n",
      "MCC for class 'Consistent': 0.6342229284856554\n",
      "MCC for class 'Inconsistent': 0.6201655777477595\n",
      "ChatGPT-4\n",
      "MCC for class 'Unrelated': 0.5479387493515656\n",
      "MCC for class 'Consistent': 0.6289176599252758\n",
      "MCC for class 'Inconsistent': 0.6099645822612041\n",
      "ChatGPT-3.5\n",
      "MCC for class 'Unrelated': 0.292490180176298\n",
      "MCC for class 'Consistent': 0.4173021985534696\n",
      "MCC for class 'Inconsistent': 0.36723945318802514\n",
      "LLaMA 8B\n",
      "MCC for class 'Unrelated': 0.127842448832716\n",
      "MCC for class 'Consistent': 0.3157350332063401\n",
      "MCC for class 'Inconsistent': 0.19465537990682572\n",
      "LLaMA 70B\n",
      "MCC for class 'Unrelated': 0.5170274762429716\n",
      "MCC for class 'Consistent': 0.705692274669214\n",
      "MCC for class 'Inconsistent': 0.6241615705543863\n",
      "MCC for class 'Unrelated': 0.7271250758242624\n",
      "MCC for class 'Consistent': 0.7982638493044966\n",
      "MCC for class 'Inconsistent': 0.786103408719816\n"
     ]
    }
   ],
   "source": [
    "matthews_corrcoef_5_runs_per_class_3_classes = dict()\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_5_runs_3_classes:\n",
    "    \n",
    "    print(model_type)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_5_runs_3_classes[model_type]:\n",
    "        y_true.append(ground_truth)\n",
    "        y_pred.append(prediction)\n",
    "            \n",
    "    # Compute MCC for each class\n",
    "    mcc_scores = []\n",
    "    for class_name in classes_3:\n",
    "        # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "        y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "        y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "        \n",
    "        # Calculate MCC for this class\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "\n",
    "    # Output MCC for each class\n",
    "    for class_name, mcc in zip(classes_3, mcc_scores):\n",
    "        print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "        \n",
    "    matthews_corrcoef_5_runs_per_class_3_classes[model_type] = dict(zip(classes_3, mcc_scores))\n",
    "\n",
    "\n",
    "model_type = 'bootstrap'\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples_3_classes:\n",
    "    y_true.append(ground_truth)\n",
    "    y_pred.append(prediction)\n",
    "    \n",
    "\n",
    "# Compute MCC for each class\n",
    "mcc_scores = []\n",
    "for class_name in classes_3:\n",
    "    # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "    y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "    y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "    \n",
    "    # Calculate MCC for this class\n",
    "    mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "# Output MCC for each class\n",
    "for class_name, mcc in zip(classes_3, mcc_scores):\n",
    "    print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "    \n",
    "matthews_corrcoef_5_runs_per_class_3_classes[model_type] = dict(zip(classes_3, mcc_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/matthews_corrcoef_5_runs_per_class_3_classes.pkl', 'wb') as f:\n",
    "    pickle.dump(matthews_corrcoef_5_runs_per_class_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "MCC for class 'Unrelated': 0.4898683723899008\n",
      "MCC for class 'Consistent': 0.6240924859438436\n",
      "MCC for class 'Indirect inconsistency': 0.2470729047013832\n",
      "MCC for class 'Factual inconsistency': 0.26115717607945876\n",
      "MCC for class 'Surface contradiction': 0.4125043198569154\n",
      "ChatGPT-4\n",
      "MCC for class 'Unrelated': 0.5336216117558299\n",
      "MCC for class 'Consistent': 0.6269120062815203\n",
      "MCC for class 'Indirect inconsistency': 0.17552635065737107\n",
      "MCC for class 'Factual inconsistency': 0.22042918435148218\n",
      "MCC for class 'Surface contradiction': 0.3272980890203094\n",
      "ChatGPT-3.5\n",
      "MCC for class 'Unrelated': 0.2741280801376437\n",
      "MCC for class 'Consistent': 0.403543239303218\n",
      "MCC for class 'Indirect inconsistency': 0.0931470037114452\n",
      "MCC for class 'Factual inconsistency': 0.16758393613193637\n",
      "MCC for class 'Surface contradiction': 0.17050173166598745\n",
      "LLaMA 8B\n",
      "MCC for class 'Unrelated': 0.13503134353375384\n",
      "MCC for class 'Consistent': 0.3104549563718792\n",
      "MCC for class 'Indirect inconsistency': 0.0027876131040327264\n",
      "MCC for class 'Factual inconsistency': 0.06544788128542957\n",
      "MCC for class 'Surface contradiction': 0.056964003084035465\n",
      "LLaMA 70B\n",
      "MCC for class 'Unrelated': 0.5072116118870345\n",
      "MCC for class 'Consistent': 0.6862249958343121\n",
      "MCC for class 'Indirect inconsistency': 0.28038668503825415\n",
      "MCC for class 'Factual inconsistency': 0.20451725892018205\n",
      "MCC for class 'Surface contradiction': 0.39537977837882704\n",
      "MCC for class 'Unrelated': 0.7157325874971767\n",
      "MCC for class 'Consistent': 0.7857342723881804\n",
      "MCC for class 'Indirect inconsistency': 0.5906623958002127\n",
      "MCC for class 'Factual inconsistency': 0.57283963807414\n",
      "MCC for class 'Surface contradiction': 0.6749434614250672\n"
     ]
    }
   ],
   "source": [
    "matthews_corrcoef_5_runs_per_class = dict()\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_5_runs:\n",
    "    \n",
    "    print(model_type)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_5_runs[model_type]:\n",
    "        y_true.append(ground_truth)\n",
    "        y_pred.append(prediction)\n",
    "            \n",
    "    # Compute MCC for each class\n",
    "    mcc_scores = []\n",
    "    for class_name in classes_5:\n",
    "        # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "        y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "        y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "        \n",
    "        # Calculate MCC for this class\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "\n",
    "    # Output MCC for each class\n",
    "    for class_name, mcc in zip(classes_5, mcc_scores):\n",
    "        print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "        \n",
    "    matthews_corrcoef_5_runs_per_class[model_type] = dict(zip(classes_5, mcc_scores))\n",
    "\n",
    "\n",
    "model_type = 'bootstrap'\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples:\n",
    "    y_true.append(ground_truth)\n",
    "    y_pred.append(prediction)\n",
    "    \n",
    "\n",
    "# Compute MCC for each class\n",
    "mcc_scores = []\n",
    "for class_name in classes_5:\n",
    "    # Convert labels to binary for one-vs-all (True if the sample is the current class, False otherwise)\n",
    "    y_true_binary = np.array([1 if label == class_name else 0 for label in y_true])\n",
    "    y_pred_binary = np.array([1 if label == class_name else 0 for label in y_pred])\n",
    "    \n",
    "    # Calculate MCC for this class\n",
    "    mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "# Output MCC for each class\n",
    "for class_name, mcc in zip(classes_5, mcc_scores):\n",
    "    print(f\"MCC for class '{class_name}': {mcc}\")\n",
    "    \n",
    "matthews_corrcoef_5_runs_per_class[model_type] = dict(zip(classes_5, mcc_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/matthews_corrcoef_5_runs_per_class.pkl', 'wb') as f:\n",
    "    pickle.dump(matthews_corrcoef_5_runs_per_class, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_precision_majority_LLM_3_classses = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_majority_LLM_3_classes:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_majority_LLM_3_classes[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "    macro = precision_score(true_labels, predictions, average='macro')\n",
    "    final_precision_majority_LLM_3_classses[model] = dict(zip(classes_3, precision))\n",
    "    final_precision_majority_LLM_3_classses[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples_3_classes\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "precision = precision_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "macro = precision_score(true_labels, predictions, average='macro')\n",
    "final_precision_majority_LLM_3_classses[model] = dict(zip(classes_3, precision))\n",
    "final_precision_majority_LLM_3_classses[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_precision_majority_LLM_3_classses.pkl', 'wb') as f:\n",
    "    pickle.dump(final_precision_majority_LLM_3_classses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_precision_majority_LLM = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_majority_LLM:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_majority_LLM[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "    macro = precision_score(true_labels, predictions, average='macro')\n",
    "    final_precision_majority_LLM[model] = dict(zip(classes_5, precision))\n",
    "    final_precision_majority_LLM[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "precision = precision_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "macro = precision_score(true_labels, predictions, average='macro')\n",
    "final_precision_majority_LLM[model] = dict(zip(classes_5, precision))\n",
    "final_precision_majority_LLM[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_precision_majority_LLM.pkl', 'wb') as f:\n",
    "    pickle.dump(final_precision_majority_LLM, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_precision_5_runs_3_classses = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_5_runs_3_classes:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_5_runs_3_classes[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "    macro = precision_score(true_labels, predictions, average='macro')\n",
    "    final_precision_5_runs_3_classses[model] = dict(zip(classes_3, precision))\n",
    "    final_precision_5_runs_3_classses[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples_3_classes\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "precision = precision_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "macro = precision_score(true_labels, predictions, average='macro')\n",
    "final_precision_5_runs_3_classses[model] = dict(zip(classes_3, precision))\n",
    "final_precision_5_runs_3_classses[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_precision_5_runs_3_classses.pkl', 'wb') as f:\n",
    "    pickle.dump(final_precision_5_runs_3_classses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_precision_5_runs = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_5_runs:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_5_runs[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    precision = precision_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "    macro = precision_score(true_labels, predictions, average='macro')\n",
    "    final_precision_5_runs[model] = dict(zip(classes_5, precision))\n",
    "    final_precision_5_runs[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "precision = precision_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "macro = precision_score(true_labels, predictions, average='macro')\n",
    "final_precision_5_runs[model] = dict(zip(classes_5, precision))\n",
    "final_precision_5_runs[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_precision_5_runs.pkl', 'wb') as f:\n",
    "    pickle.dump(final_precision_5_runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_recall_majority_LLM_3_classses = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_majority_LLM_3_classes:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_majority_LLM_3_classes[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    recall = recall_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "    macro = recall_score(true_labels, predictions, average='macro')\n",
    "    final_recall_majority_LLM_3_classses[model] = dict(zip(classes_3, recall))\n",
    "    final_recall_majority_LLM_3_classses[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples_3_classes\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "recall = recall_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "macro = recall_score(true_labels, predictions, average='macro')\n",
    "final_recall_majority_LLM_3_classses[model] = dict(zip(classes_3, recall))\n",
    "final_recall_majority_LLM_3_classses[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_recall_majority_LLM_3_classses.pkl', 'wb') as f:\n",
    "    pickle.dump(final_recall_majority_LLM_3_classses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_recall_majority_LLM = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_majority_LLM:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_majority_LLM[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    recall = recall_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "    macro = recall_score(true_labels, predictions, average='macro')\n",
    "    final_recall_majority_LLM[model] = dict(zip(classes_5, recall))\n",
    "    final_recall_majority_LLM[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "recall = recall_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "macro = recall_score(true_labels, predictions, average='macro')\n",
    "final_recall_majority_LLM[model] = dict(zip(classes_5, recall))\n",
    "final_recall_majority_LLM[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_recall_majority_LLM.pkl', 'wb') as f:\n",
    "    pickle.dump(final_recall_majority_LLM, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_recall_5_runs_3_classses = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_5_runs_3_classes:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_5_runs_3_classes[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    recall = recall_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "    macro = recall_score(true_labels, predictions, average='macro')\n",
    "    final_recall_5_runs_3_classses[model] = dict(zip(classes_3, recall))\n",
    "    final_recall_5_runs_3_classses[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples_3_classes\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "recall = recall_score(true_labels, predictions,labels=['Unrelated', 'Consistent', 'Inconsistent'], average=None)\n",
    "macro = recall_score(true_labels, predictions, average='macro')\n",
    "final_recall_5_runs_3_classses[model] = dict(zip(classes_3, recall))\n",
    "final_recall_5_runs_3_classses[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_recall_5_runs_3_classses.pkl', 'wb') as f:\n",
    "    pickle.dump(final_recall_5_runs_3_classses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "ChatGPT-4\n",
      "ChatGPT-3.5\n",
      "LLaMA 8B\n",
      "LLaMA 70B\n",
      "bootstrap\n"
     ]
    }
   ],
   "source": [
    "final_recall_5_runs = dict()\n",
    "\n",
    "for model in prediction_ground_truth_tuples_5_runs:\n",
    "    print(model)\n",
    "    answers = prediction_ground_truth_tuples_5_runs[model]\n",
    "    true_labels = [el[0] for el in answers] \n",
    "    predictions = [el[1] for el in answers]\n",
    "    \n",
    "    recall = recall_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "    macro = recall_score(true_labels, predictions, average='macro')\n",
    "    final_recall_5_runs[model] = dict(zip(classes_5, recall))\n",
    "    final_recall_5_runs[model]['macro'] = macro\n",
    "\n",
    "model = 'bootstrap'\n",
    "print(model)\n",
    "answers = bootstrap_predictions_tuples\n",
    "true_labels = [el[0] for el in answers] \n",
    "predictions = [el[1] for el in answers]\n",
    "recall = recall_score(true_labels, predictions,labels=classes_5, average=None)\n",
    "macro = recall_score(true_labels, predictions, average='macro')\n",
    "final_recall_5_runs[model] = dict(zip(classes_5, recall))\n",
    "final_recall_5_runs[model]['macro'] = macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/final_recall_5_runs.pkl', 'wb') as f:\n",
    "    pickle.dump(final_recall_5_runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score for majority LLM - 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "Per class [0.58319039 0.68117519 0.88539778]\n",
      "Just mean 0.716587789808988\n",
      "Macro 0.716587789808988\n",
      "Micro 0.8087943262411349\n",
      "ChatGPT-4\n",
      "Per class [0.62       0.7026455  0.87325702]\n",
      "Just mean 0.731967508859635\n",
      "Macro 0.731967508859635\n",
      "Micro 0.8036879432624113\n",
      "ChatGPT-3.5\n",
      "Per class [0.1986755  0.41463415 0.86750681]\n",
      "Just mean 0.49360548500643536\n",
      "Macro 0.49360548500643536\n",
      "Micro 0.7733333333333333\n",
      "LLaMA 8B\n",
      "Per class [0.21551724 0.34756554 0.6726674 ]\n",
      "Just mean 0.41191672763789944\n",
      "Macro 0.41191672763789944\n",
      "Micro 0.535886524822695\n",
      "LLaMA 70B\n",
      "Per class [0.55253837 0.74193548 0.90612555]\n",
      "Just mean 0.7335331332738586\n",
      "Macro 0.7335331332738586\n",
      "Micro 0.8419858156028368\n"
     ]
    }
   ],
   "source": [
    "f1_score_per_model_type_majority_LLM_3_classes = dict()\n",
    "\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_majority_LLM_3_classes:\n",
    "    f1_score_per_model_type_majority_LLM_3_classes[model_type] = dict()\n",
    "    print(model_type)\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_majority_LLM_3_classes[model_type]:\n",
    "        true_labels.append(ground_truth)\n",
    "        predictions.append(prediction)\n",
    "    res_f1_score = f1_score(true_labels, predictions, labels=classes_3, average=None)\n",
    "    print(\"Per class\", res_f1_score)\n",
    "    my_dict = dict(zip(classes_3, res_f1_score))\n",
    "    f1_score_per_model_type_majority_LLM_3_classes[model_type] = my_dict\n",
    "    print('Just mean', np.mean(res_f1_score))\n",
    "    res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_3, average='macro')\n",
    "    print(\"Macro\", res_f1_score_macro)\n",
    "    f1_score_per_model_type_majority_LLM_3_classes[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "    res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_3, average='micro')\n",
    "    print(\"Micro\", res_f1_score_micro)\n",
    "    f1_score_per_model_type_majority_LLM_3_classes[model_type]['micro'] = res_f1_score_micro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class [0.76945039 0.82290398 0.93802598]\n",
      "Just mean 0.8434601173271709\n",
      "Macro 0.8434601173271709\n",
      "Micro 0.8964822695035461\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bootstrap'\n",
    "f1_score_per_model_type_majority_LLM_3_classes['bootstrap'] = dict()\n",
    "true_labels = []\n",
    "predictions = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples_3_classes:\n",
    "    true_labels.append(ground_truth)\n",
    "    predictions.append(prediction)\n",
    "res_f1_score = f1_score(true_labels, predictions, labels=classes_3, average=None)\n",
    "print(\"Per class\", res_f1_score)\n",
    "my_dict = dict(zip(classes_3, res_f1_score))\n",
    "f1_score_per_model_type_majority_LLM_3_classes[model_type] = my_dict\n",
    "print('Just mean', np.mean(res_f1_score))\n",
    "res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_3, average='macro')\n",
    "print(\"Macro\", res_f1_score_macro)\n",
    "f1_score_per_model_type_majority_LLM_3_classes[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_3, average='micro')\n",
    "print(\"Micro\", res_f1_score_micro)\n",
    "f1_score_per_model_type_majority_LLM_3_classes[model_type]['micro'] = res_f1_score_micro\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/f1_score_per_model_type_majority_LLM_3_classes.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_score_per_model_type_majority_LLM_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score for 5 runs - 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "Per class [0.59846547 0.67965368 0.88592772]\n",
      "Just mean 0.7213489577909499\n",
      "Macro 0.7213489577909499\n",
      "Micro 0.8110638297872339\n",
      "ChatGPT-4\n",
      "Per class [0.62141872 0.67434073 0.86991021]\n",
      "Just mean 0.7218898871563099\n",
      "Macro 0.7218898871563099\n",
      "Micro 0.7973900709219858\n",
      "ChatGPT-3.5\n",
      "Per class [0.18151815 0.40425532 0.86419163]\n",
      "Just mean 0.4833217013589303\n",
      "Macro 0.4833217013589303\n",
      "Micro 0.7674893617021277\n",
      "LLaMA 8B\n",
      "Per class [0.27590759 0.40278408 0.69757761]\n",
      "Just mean 0.4587564275219275\n",
      "Macro 0.4587564275219275\n",
      "Micro 0.5698156028368795\n",
      "LLaMA 70B\n",
      "Per class [0.54583723 0.74070946 0.90266026]\n",
      "Just mean 0.7297356514187122\n",
      "Macro 0.7297356514187122\n",
      "Micro 0.8376170212765958\n"
     ]
    }
   ],
   "source": [
    "f1_score_per_model_type_5_runs_3_classes = dict()\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_5_runs_3_classes:\n",
    "    f1_score_per_model_type_5_runs_3_classes[model_type] = dict()\n",
    "    print(model_type)\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_5_runs_3_classes[model_type]:\n",
    "        true_labels.append(ground_truth)\n",
    "        predictions.append(prediction)\n",
    "    res_f1_score = f1_score(true_labels, predictions, labels=classes_3, average=None)\n",
    "    print(\"Per class\", res_f1_score)\n",
    "    my_dict = dict(zip(classes_3, res_f1_score))\n",
    "    f1_score_per_model_type_5_runs_3_classes[model_type] = my_dict\n",
    "    print('Just mean', np.mean(res_f1_score))\n",
    "    res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_3, average='macro')\n",
    "    print(\"Macro\", res_f1_score_macro)\n",
    "    f1_score_per_model_type_5_runs_3_classes[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "    res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_3, average='micro')\n",
    "    print(\"Micro\", res_f1_score_micro)\n",
    "    f1_score_per_model_type_5_runs_3_classes[model_type]['micro'] = res_f1_score_micro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class [0.76945039 0.82290398 0.93802598]\n",
      "Just mean 0.8434601173271709\n",
      "Macro 0.8434601173271709\n",
      "Micro 0.8964822695035461\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bootstrap'\n",
    "f1_score_per_model_type_5_runs_3_classes['bootstrap'] = dict()\n",
    "true_labels = []\n",
    "predictions = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples_3_classes:\n",
    "    true_labels.append(ground_truth)\n",
    "    predictions.append(prediction)\n",
    "res_f1_score = f1_score(true_labels, predictions, labels=classes_3, average=None)\n",
    "print(\"Per class\", res_f1_score)\n",
    "my_dict = dict(zip(classes_3, res_f1_score))\n",
    "f1_score_per_model_type_5_runs_3_classes[model_type] = my_dict\n",
    "print('Just mean', np.mean(res_f1_score))\n",
    "res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_3, average='macro')\n",
    "print(\"Macro\", res_f1_score_macro)\n",
    "f1_score_per_model_type_5_runs_3_classes[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_3, average='micro')\n",
    "print(\"Micro\", res_f1_score_micro)\n",
    "f1_score_per_model_type_5_runs_3_classes[model_type]['micro'] = res_f1_score_micro\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/f1_score_per_model_type_5_runs_3_classes.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_score_per_model_type_5_runs_3_classes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score for 5 runs - 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "Per class [0.57928803 0.67559217 0.44311377 0.39754413 0.55431591]\n",
      "Just mean 0.5299708026780671\n",
      "Macro 0.5299708026780671\n",
      "Micro 0.5174468085106383\n",
      "ChatGPT-4\n",
      "Per class [0.61970184 0.67863555 0.45127693 0.23447591 0.31134367]\n",
      "Just mean 0.4590867798983247\n",
      "Macro 0.4590867798983247\n",
      "Micro 0.4702978723404255\n",
      "ChatGPT-3.5\n",
      "Per class [0.16442451 0.38424897 0.24084527 0.35903064 0.41092375]\n",
      "Just mean 0.31189462912921684\n",
      "Macro 0.31189462912921684\n",
      "Micro 0.3357163120567376\n",
      "LLaMA 8B\n",
      "Per class [0.29113725 0.41291642 0.16266945 0.24903503 0.27922624]\n",
      "Just mean 0.2789968781886877\n",
      "Macro 0.2789968781886877\n",
      "Micro 0.27914893617021275\n",
      "LLaMA 70B\n",
      "Per class [0.53016772 0.7294307  0.509321   0.24245305 0.48415156]\n",
      "Just mean 0.4991048046691481\n",
      "Macro 0.4991048046691481\n",
      "Micro 0.5063829787234042\n"
     ]
    }
   ],
   "source": [
    "f1_score_per_model_type_5_runs = dict()\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_5_runs:\n",
    "    f1_score_per_model_type_5_runs[model_type] = dict()\n",
    "    print(model_type)\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_5_runs[model_type]:\n",
    "        true_labels.append(ground_truth)\n",
    "        predictions.append(prediction)\n",
    "    res_f1_score = f1_score(true_labels, predictions, labels=classes_5, average=None)\n",
    "    print(\"Per class\", res_f1_score)\n",
    "    my_dict = dict(zip(classes_5, res_f1_score))\n",
    "    f1_score_per_model_type_5_runs[model_type] = my_dict\n",
    "    print('Just mean', np.mean(res_f1_score))\n",
    "    res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_5, average='macro')\n",
    "    print(\"Macro\", res_f1_score_macro)\n",
    "    f1_score_per_model_type_5_runs[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "    res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_5, average='micro')\n",
    "    print(\"Micro\", res_f1_score_micro)\n",
    "    f1_score_per_model_type_5_runs[model_type]['micro'] = res_f1_score_micro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class [0.76555798 0.81462905 0.69751779 0.6507764  0.7545423 ]\n",
      "Just mean 0.7366047033160935\n",
      "Macro 0.7366047033160935\n",
      "Micro 0.7307801418439717\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bootstrap'\n",
    "f1_score_per_model_type_5_runs['bootstrap'] = dict()\n",
    "true_labels = []\n",
    "predictions = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples:\n",
    "    true_labels.append(ground_truth)\n",
    "    predictions.append(prediction)\n",
    "res_f1_score = f1_score(true_labels, predictions, labels=classes_5, average=None)\n",
    "print(\"Per class\", res_f1_score)\n",
    "my_dict = dict(zip(classes_5, res_f1_score))\n",
    "f1_score_per_model_type_5_runs[model_type] = my_dict\n",
    "print('Just mean', np.mean(res_f1_score))\n",
    "res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_5, average='macro')\n",
    "print(\"Macro\", res_f1_score_macro)\n",
    "f1_score_per_model_type_5_runs[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_5, average='micro')\n",
    "print(\"Micro\", res_f1_score_micro)\n",
    "f1_score_per_model_type_5_runs[model_type]['micro'] = res_f1_score_micro\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/f1_score_per_model_type_5_runs.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_score_per_model_type_5_runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score for majority LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humans\n",
      "Per class [0.5984     0.67708333 0.44347351 0.39227799 0.55892649]\n",
      "Just mean 0.5340322650203002\n",
      "Macro 0.5340322650203002\n",
      "Micro 0.5214184397163121\n",
      "ChatGPT-4\n",
      "Per class [0.63294798 0.71399594 0.44950213 0.19768935 0.31588613]\n",
      "Just mean 0.4620043066357198\n",
      "Macro 0.4620043066357198\n",
      "Micro 0.47404255319148936\n",
      "ChatGPT-3.5\n",
      "Per class [0.1744186  0.39349593 0.22979985 0.35601118 0.43161634]\n",
      "Just mean 0.31706838319602587\n",
      "Macro 0.31706838319602587\n",
      "Micro 0.3415602836879433\n",
      "LLaMA 8B\n",
      "Per class [0.21864952 0.36337209 0.13168087 0.16833333 0.27127385]\n",
      "Just mean 0.230661932817461\n",
      "Macro 0.230661932817461\n",
      "Micro 0.23687943262411348\n",
      "LLaMA 70B\n",
      "Per class [0.53061224 0.73738414 0.50658561 0.25298329 0.47294292]\n",
      "Just mean 0.500101642433283\n",
      "Macro 0.500101642433283\n",
      "Micro 0.5049645390070922\n"
     ]
    }
   ],
   "source": [
    "f1_score_per_model_type_majority_LLM = dict()\n",
    "\n",
    "\n",
    "for model_type in prediction_ground_truth_tuples_majority_LLM:\n",
    "    f1_score_per_model_type_majority_LLM[model_type] = dict()\n",
    "    print(model_type)\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    for ground_truth, prediction in prediction_ground_truth_tuples_majority_LLM[model_type]:\n",
    "        true_labels.append(ground_truth)\n",
    "        predictions.append(prediction)\n",
    "    res_f1_score = f1_score(true_labels, predictions, labels=classes_5, average=None)\n",
    "    print(\"Per class\", res_f1_score)\n",
    "    my_dict = dict(zip(classes_5, res_f1_score))\n",
    "    f1_score_per_model_type_majority_LLM[model_type] = my_dict\n",
    "    print('Just mean', np.mean(res_f1_score))\n",
    "    res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_5, average='macro')\n",
    "    print(\"Macro\", res_f1_score_macro)\n",
    "    f1_score_per_model_type_majority_LLM[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "    res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_5, average='micro')\n",
    "    print(\"Micro\", res_f1_score_micro)\n",
    "    f1_score_per_model_type_majority_LLM[model_type]['micro'] = res_f1_score_micro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class [0.76555798 0.81462905 0.69751779 0.6507764  0.7545423 ]\n",
      "Just mean 0.7366047033160935\n",
      "Macro 0.7366047033160935\n",
      "Micro 0.7307801418439717\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bootstrap'\n",
    "f1_score_per_model_type_majority_LLM['bootstrap'] = dict()\n",
    "true_labels = []\n",
    "predictions = []\n",
    "for ground_truth, prediction in bootstrap_predictions_tuples:\n",
    "    true_labels.append(ground_truth)\n",
    "    predictions.append(prediction)\n",
    "res_f1_score = f1_score(true_labels, predictions, labels=classes_5, average=None)\n",
    "print(\"Per class\", res_f1_score)\n",
    "my_dict = dict(zip(classes_5, res_f1_score))\n",
    "f1_score_per_model_type_majority_LLM[model_type] = my_dict\n",
    "print('Just mean', np.mean(res_f1_score))\n",
    "res_f1_score_macro = f1_score(true_labels, predictions, labels=classes_5, average='macro')\n",
    "print(\"Macro\", res_f1_score_macro)\n",
    "f1_score_per_model_type_majority_LLM[model_type]['macro'] = res_f1_score_macro\n",
    "\n",
    "res_f1_score_micro = f1_score(true_labels, predictions, labels=classes_5, average='micro')\n",
    "print(\"Micro\", res_f1_score_micro)\n",
    "f1_score_per_model_type_majority_LLM[model_type]['micro'] = res_f1_score_micro\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/model_evaluations/f1_score_per_model_type_majority_LLM.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_score_per_model_type_majority_LLM, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"sk-proj-3KhQdlgMCuoQc6EL2y1ZT3BlbkFJYHG9JgPGpOLsRnDSKuSs\")\n",
    "prompt_5_types = \"asst_13QqvGJEmZcaizKEmeSZMQnC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_3_types = \"asst_zX008FyufXH0pgwjuUXAcZkX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text):\n",
    "\n",
    "    try:\n",
    "        # Create a thread with a message.\n",
    "        thread = client.beta.threads.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    # Update this with the query you want to use.\n",
    "                    \"content\": text,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    # Submit the thread to the assistant (as a new run).\n",
    "        run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=prompt_5_types)\n",
    "\n",
    "        # Wait for run to complete.\n",
    "        while run.status != \"completed\":\n",
    "            run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "            time.sleep(1)\n",
    "        # else:\n",
    "        #     print(f\"üèÅ Run Completed!\")\n",
    "\n",
    "        # Get the latest message from the thread.\n",
    "        message_response = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        messages = message_response.data\n",
    "\n",
    "        # Print the latest message.\n",
    "        latest_message = messages[0]\n",
    "        return latest_message.content[0].text.value\n",
    "    except:\n",
    "        print(f\"Run went wrong\")\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_and_explanation(text):\n",
    "    label_match = re.search(r\"Label:(.*?)Explanation:\", text, re.DOTALL)\n",
    "    label_text = label_match.group(1).strip() if label_match else text\n",
    "\n",
    "    # Extract text after \"Explanation\"\n",
    "    explanation_match = re.search(r\"Explanation:(.*)\", text, re.DOTALL)\n",
    "    explanation_text = explanation_match.group(1).strip() if explanation_match else text\n",
    "    \n",
    "    return label_text, explanation_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun experiment 3 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = pd.read_csv(r\"C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\data\\qualtrics_survey\\results\\df_gold_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = list(df_gold['Input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_run/answers_chatgpt4_turbo_5_types_run5.pkl', 'rb') as f:\n",
    "    answers_chatgpt4_turbo_5_types_run5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 698/698 [38:43<00:00,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# answers_chatgpt4_turbo_5_types_run5 = dict()\n",
    "for sample in tqdm(statements):\n",
    "    if sample not in answers_chatgpt4_turbo_5_types_run5:\n",
    "        answers_chatgpt4_turbo_5_types_run5[sample] = dict()\n",
    "        answer = predict_label(sample)\n",
    "        try:\n",
    "            label, explanation = extract_label_and_explanation(answer)\n",
    "            answers_chatgpt4_turbo_5_types_run5[sample]['label'] = label\n",
    "            answers_chatgpt4_turbo_5_types_run5[sample]['explanation'] = explanation\n",
    "        except:\n",
    "            print(\"Couldn't parse the answer for sample\")\n",
    "            print(sample)\n",
    "            answers_chatgpt4_turbo_5_types_run5[sample]['label'] = answer\n",
    "        with open('new_run/answers_chatgpt4_turbo_5_types_run5.pkl', 'wb') as f:\n",
    "            pickle.dump(answers_chatgpt4_turbo_5_types_run5, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_chatgpt35_turbo_5_types_run4 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_rerun = [\"Text 1: The states of Berlin and Brandenburg have enough to handle with the major project of Airport and its skyrocketing costs at the expense of taxpayers.\\n\\nText 2: We believe Brandenburg should back Berlin's bid for the Olympic Games.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\notebooks\\evaluating_models\\new_run\\answers_chatgpt4_turbo_5_types_run5.pkl', 'rb') as f:\n",
    "    chatgpt4_run_5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.68s/it]\n"
     ]
    }
   ],
   "source": [
    "for sample in tqdm(samples_to_rerun):\n",
    "    chatgpt4_run_5[sample] = dict()\n",
    "    answer = predict_label(sample)\n",
    "    try:\n",
    "        label, explanation = extract_label_and_explanation(answer)\n",
    "        chatgpt4_run_5[sample]['label'] = label\n",
    "        chatgpt4_run_5[sample]['explanation'] = explanation\n",
    "    except:\n",
    "        print(\"Couldn't parse the answer for sample\")\n",
    "        print(sample)\n",
    "        chatgpt4_run_5[sample]['label'] = answer\n",
    "    with open('new_run/answers_chatgpt4_turbo_5_types_run5.pkl', 'wb') as f:\n",
    "        pickle.dump(chatgpt4_run_5, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_run/answers_chatgpt35_turbo_5_types_run2.pkl', 'wb') as f:\n",
    "        pickle.dump(answers_chatgpt35_turbo_5_types_run2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 698/698 [54:26<00:00,  4.68s/it] \n"
     ]
    }
   ],
   "source": [
    "# answers_chatgpt35_turbo_5_types_run2 = dict()\n",
    "for sample in tqdm(statements):\n",
    "    answers_chatgpt35_turbo_5_types_run2[sample] = dict()\n",
    "    answer = predict_label(sample)\n",
    "    try:\n",
    "        label, explanation = extract_label_and_explanation(answer)\n",
    "        answers_chatgpt35_turbo_5_types_run2[sample]['label'] = label\n",
    "        answers_chatgpt35_turbo_5_types_run2[sample]['explanation'] = explanation\n",
    "    except:\n",
    "        print(\"Couldn't parse the answer for sample\")\n",
    "        print(sample)\n",
    "        answers_chatgpt35_turbo_5_types_run2[sample]['label'] = answer\n",
    "    with open('new_run/answers_chatgpt35_turbo_5_types_run2.pkl', 'wb') as f:\n",
    "        pickle.dump(answers_chatgpt35_turbo_5_types_run2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_3_types(text):\n",
    "\n",
    "    try:\n",
    "        # Create a thread with a message.\n",
    "        thread = client.beta.threads.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    # Update this with the query you want to use.\n",
    "                    \"content\": text,\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    # Submit the thread to the assistant (as a new run).\n",
    "        run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=prompt_3_types)\n",
    "\n",
    "        # Wait for run to complete.\n",
    "        while run.status != \"completed\":\n",
    "            run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "            time.sleep(1)\n",
    "        # else:\n",
    "        #     print(f\"üèÅ Run Completed!\")\n",
    "\n",
    "        # Get the latest message from the thread.\n",
    "        message_response = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        messages = message_response.data\n",
    "\n",
    "        # Print the latest message.\n",
    "        latest_message = messages[0]\n",
    "        return latest_message.content[0].text.value\n",
    "    except:\n",
    "        print(f\"Run went wrong\")\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\notebooks\\sample_to_responses_Part_1.pkl\", 'rb') as f:\n",
    "    sample_to_responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\notebooks\\evaluating_models\\answers_chatgpt_3_types.pkl', 'rb') as f:\n",
    "    answers_chatgpt_3_types = pickle.load(f)\n",
    "with open(r'C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\notebooks\\evaluating_models\\answers_chatgpt_3_types_350_700.pkl', 'rb') as f:\n",
    "    answers_chatgpt_3_types_pt2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace \"\\n\" with \"\\n\\n\"\n",
    "key_changes = dict()\n",
    "for el in answers_chatgpt_3_types_pt2.keys():\n",
    "    key_changes[el] = el.replace(\"\\n\", \"\\n\\n\")\n",
    "    \n",
    "# Rename keys\n",
    "for old_key, new_key in key_changes.items():\n",
    "    if old_key in answers_chatgpt_3_types_pt2:\n",
    "        answers_chatgpt_3_types_pt2[new_key] = answers_chatgpt_3_types_pt2.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_samples = []\n",
    "for el in answers_chatgpt_3_types:\n",
    "    annotated_samples.append(el)\n",
    "for el in answers_chatgpt_3_types_pt2:\n",
    "    annotated_samples.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\data\\qualtrics_survey\\all_samples_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = pd.read_csv(r\"C:\\Users\\Nursulu_1\\Downloads\\ContraDetect\\automatic_stance_detection\\data\\qualtrics_survey\\results\\df_gold_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>All answers</th>\n",
       "      <th>Answer_3_classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text 1: Motivated and confident workforce is a...</td>\n",
       "      <td>Indirect inconsistency</td>\n",
       "      <td>80.000</td>\n",
       "      <td>['Indirect inconsistency', 'Indirect inconsist...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text 1: We want broad support for agriculture ...</td>\n",
       "      <td>Surface contradiction</td>\n",
       "      <td>60.000</td>\n",
       "      <td>['Consistent', 'Surface contradiction', 'Surfa...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text 1: There should be a ‚Ç¨29 ticket for every...</td>\n",
       "      <td>Surface contradiction</td>\n",
       "      <td>80.000</td>\n",
       "      <td>['Indirect inconsistency', 'Surface contradict...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Text 1: We support the implementation of pilot...</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>60.000</td>\n",
       "      <td>['Indirect inconsistency', 'Unrelated', 'Factu...</td>\n",
       "      <td>Unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text 1: We fully support the conversion of add...</td>\n",
       "      <td>Indirect inconsistency</td>\n",
       "      <td>83.333</td>\n",
       "      <td>['Unrelated', 'Indirect inconsistency', 'Indir...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Text 1:  To bolster the economy, a major focus...</td>\n",
       "      <td>Indirect inconsistency</td>\n",
       "      <td>60.000</td>\n",
       "      <td>['Indirect inconsistency', 'Indirect inconsist...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Text 1: Students should have the free choice b...</td>\n",
       "      <td>Surface contradiction</td>\n",
       "      <td>60.000</td>\n",
       "      <td>['Factual inconsistency', 'Factual inconsisten...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Text 1: Belonging to a certain social or ethni...</td>\n",
       "      <td>Indirect inconsistency</td>\n",
       "      <td>80.000</td>\n",
       "      <td>['Indirect inconsistency', 'Indirect inconsist...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Text 1: The level of social basic security mus...</td>\n",
       "      <td>Factual inconsistency</td>\n",
       "      <td>40.000</td>\n",
       "      <td>['Indirect inconsistency', 'Factual inconsiste...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Text 1: We advocate for banning helicopter fli...</td>\n",
       "      <td>Factual inconsistency</td>\n",
       "      <td>60.000</td>\n",
       "      <td>['Surface contradiction', 'Surface contradicti...</td>\n",
       "      <td>Inconsistent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>698 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Input  \\\n",
       "0    Text 1: Motivated and confident workforce is a...   \n",
       "1    Text 1: We want broad support for agriculture ...   \n",
       "2    Text 1: There should be a ‚Ç¨29 ticket for every...   \n",
       "3    Text 1: We support the implementation of pilot...   \n",
       "4    Text 1: We fully support the conversion of add...   \n",
       "..                                                 ...   \n",
       "693  Text 1:  To bolster the economy, a major focus...   \n",
       "694  Text 1: Students should have the free choice b...   \n",
       "695  Text 1: Belonging to a certain social or ethni...   \n",
       "696  Text 1: The level of social basic security mus...   \n",
       "697  Text 1: We advocate for banning helicopter fli...   \n",
       "\n",
       "                     Answer  Confidence  \\\n",
       "0    Indirect inconsistency      80.000   \n",
       "1     Surface contradiction      60.000   \n",
       "2     Surface contradiction      80.000   \n",
       "3                 Unrelated      60.000   \n",
       "4    Indirect inconsistency      83.333   \n",
       "..                      ...         ...   \n",
       "693  Indirect inconsistency      60.000   \n",
       "694   Surface contradiction      60.000   \n",
       "695  Indirect inconsistency      80.000   \n",
       "696   Factual inconsistency      40.000   \n",
       "697   Factual inconsistency      60.000   \n",
       "\n",
       "                                           All answers Answer_3_classes  \n",
       "0    ['Indirect inconsistency', 'Indirect inconsist...     Inconsistent  \n",
       "1    ['Consistent', 'Surface contradiction', 'Surfa...     Inconsistent  \n",
       "2    ['Indirect inconsistency', 'Surface contradict...     Inconsistent  \n",
       "3    ['Indirect inconsistency', 'Unrelated', 'Factu...        Unrelated  \n",
       "4    ['Unrelated', 'Indirect inconsistency', 'Indir...     Inconsistent  \n",
       "..                                                 ...              ...  \n",
       "693  ['Indirect inconsistency', 'Indirect inconsist...     Inconsistent  \n",
       "694  ['Factual inconsistency', 'Factual inconsisten...     Inconsistent  \n",
       "695  ['Indirect inconsistency', 'Indirect inconsist...     Inconsistent  \n",
       "696  ['Indirect inconsistency', 'Factual inconsiste...     Inconsistent  \n",
       "697  ['Surface contradiction', 'Surface contradicti...     Inconsistent  \n",
       "\n",
       "[698 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = list(df_gold['Input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(statements) - set(annotated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_annotate = list(set(statements) - set(annotated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text 1: Europe needs tax cuts, not new taxes.\\n\\nText 2: We support the EU having the ability to levy its own taxes instead of relying on national contributions.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_annotate[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [17:16<00:00, 10.16s/it]\n"
     ]
    }
   ],
   "source": [
    "answers_chatgpt4_3_types = dict()\n",
    "for sample in tqdm(to_annotate):\n",
    "    answers_chatgpt4_3_types[sample] = dict()\n",
    "    answer = predict_label_3_types(sample)\n",
    "    try:\n",
    "        label, explanation = extract_label_and_explanation(answer)\n",
    "        answers_chatgpt4_3_types[sample]['label'] = label\n",
    "        answers_chatgpt4_3_types[sample]['explanation'] = explanation\n",
    "    except:\n",
    "        print(\"Couldn't parse the answer for sample\")\n",
    "        print(sample)\n",
    "        answers_chatgpt4_3_types[sample]['label'] = answer\n",
    "    with open('answers_chatgpt4_3_types_leftovers.pkl', 'wb') as f:\n",
    "        pickle.dump(answers_chatgpt4_3_types, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 700/700 [51:24<00:00,  4.41s/it] \n"
     ]
    }
   ],
   "source": [
    "answers_chatgpt35_turbo_3_types = dict()\n",
    "for sample in tqdm(statements):\n",
    "    answers_chatgpt35_turbo_3_types[sample] = dict()\n",
    "    answer = predict_label_3_types(sample)\n",
    "    try:\n",
    "        label, explanation = extract_label_and_explanation(answer)\n",
    "        answers_chatgpt35_turbo_3_types[sample]['label'] = label\n",
    "        answers_chatgpt35_turbo_3_types[sample]['explanation'] = explanation\n",
    "    except:\n",
    "        print(\"Couldn't parse the answer for sample\")\n",
    "        print(sample)\n",
    "        answers_chatgpt35_turbo_3_types[sample]['label'] = answer\n",
    "    with open('answers_chatgpt35_turbo_3_types.pkl', 'wb') as f:\n",
    "        pickle.dump(answers_chatgpt35_turbo_3_types, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 700/700 [49:19<00:00,  4.23s/it] \n"
     ]
    }
   ],
   "source": [
    "answers_chatgpt35_turbo_5_types = dict()\n",
    "for sample in tqdm(statements):\n",
    "    answers_chatgpt35_turbo_5_types[sample] = dict()\n",
    "    answer = predict_label(sample)\n",
    "    try:\n",
    "        label, explanation = extract_label_and_explanation(answer)\n",
    "        answers_chatgpt35_turbo_5_types[sample]['label'] = label\n",
    "        answers_chatgpt35_turbo_5_types[sample]['explanation'] = explanation\n",
    "    except:\n",
    "        print(\"Couldn't parse the answer for sample\")\n",
    "        print(sample)\n",
    "        answers_chatgpt35_turbo_5_types[sample]['label'] = answer\n",
    "    with open('answers_chatgpt35_turbo_5_types_350_700.pkl', 'wb') as f:\n",
    "        pickle.dump(answers_chatgpt35_turbo_5_types, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: run the second half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/350 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "answers_chatgpt_3_types = dict()\n",
    "for sample in tqdm(statements[350:]):\n",
    "    answers_chatgpt_3_types[sample] = dict()\n",
    "    answer = predict_label_3_types(sample)\n",
    "    try:\n",
    "        label, explanation = extract_label_and_explanation(answer)\n",
    "        answers_chatgpt_3_types[sample]['label'] = label\n",
    "        answers_chatgpt_3_types[sample]['explanation'] = explanation\n",
    "    except:\n",
    "        print(\"Couldn't parse the answer for sample\")\n",
    "        print(sample)\n",
    "        answers_chatgpt_3_types[sample]['label'] = answer\n",
    "    with open('answers_chatgpt_3_types_350_700.pkl', 'wb') as f:\n",
    "        pickle.dump(answers_chatgpt_3_types, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
